Linear Regression from scratch vs Scikit-learn
This project implements linear regression from scratch using Numpy and compaires it with sklearn's built in LinearRegression. The goal is to understand how gradient descent works under the hood and evaluate how close a manual implementation can get to sklearn's optimized results. 

In this project I used a Salary dataset from kaggle. It is from SADIQ. The dataset includes 2 columns, one being a feature and the other being the target variable. The dataset contained a column labeled "YearsExperience", this was the feature variable storing the years of experience and the other column labeleed, "Salary" composed of the salary in USD based on the years of experience. 

For the linear regression implemented by scratch I had to learn and understand gradient descent and MSE, a loss function. Essentially gradient descent works by adjusting the m and b values in the linear function y = mx + b. Firstly we set m and b as 0 and compute MAE which is calculated by multiplying 1/n (n being the number of points in the dataset) by the sum of y (the acutal real y value of the point) subtracted by y_pred (this is the prediction of the point calculated by what we set m and b as) square this find the sum for each point and you have calculated the MSE. Once we have our MSE how do we know how much to change the values of m and b to deacrease our MSE. We take the derivative of the MSE formula, this give us the gradient also known as the slope for the function. We take this and multiply it by our learning rate. Learning rate is a parameter that affects how much we change m and b. Too much of a change will skip over the best values for m and b, and too low of a learning_rate will take too long and too much resource to find out. For this project I used a learning rate of 0.01 but it differs based on the project. So multiplying the gradient by the learning rate gives us the rate we should change m and b so therefore to adjust m and b we would use the formula: m = m - learning_rate * dm. Dm is simply just the derivative of m which is the change in m which is the derivative of the MSE formula coming out to be: (-2/n) * np.sum(x*(y - y_pred)) for m and db = (-2/n) * np.sum(y - y_pred) for b. This is it our model is completelly done all we had left to do is graph it which is simple and straight forward. Next I used sklearn to build a linear model to compare with the one I made from scratch. I skim over this because it's pretty straight forward but effectivelly I split the data into x and y, split data into testing and training, trained the linear model i created based on training data. Predicted y_pred values based on x_test values. Compared y_pred and y_test using MSE. Lastly I compared their MSE, this tells us how good of a line we made, a lower MSE means a better model. The linear model from scratch got a MSE of, 36539293.32 while the sklearn linear model got a MSE of, 55761791.31. This means my linear model from scratch did better. 


